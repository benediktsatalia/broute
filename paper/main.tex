\documentclass[11pt,a4paper,notitlepage]{article}
\usepackage{amssymb}
\usepackage{amssymb}
\usepackage{geometry} 
\usepackage{float}
\usepackage{booktabs}
\usepackage{algorithm,algorithmic}
\usepackage{amsmath}
\usepackage{pdflscape}
\usepackage{url}
\usepackage{natbib}
\usepackage{graphicx,color}
\usepackage[utf8]{inputenc}
\usepackage{listings} % for inline code
\usepackage{verbatim}

\allowdisplaybreaks
\geometry{a4paper,left=1in,right=1in,top=1in,bottom=1in}

\newcommand{\ft}{\color{blue}}  % to show changes in revised version (F.T.)

\begin{document}

\title{A benchmark for the implementation of standard vehicle routing
  algorithms}
\author{
Fabien Tricoire$^{1}$ \\[1ex]
 \small $^1$Institute for Transport and Logistics Management,
 Vienna University of Economics and Business\\
 \small Welthandelsplatz 1, 1020 Vienna, Austria\\
  \small \texttt{fabien.tricoire@wu.ac.at}\\[2ex]
}
\date{}
\maketitle

\begin{abstract}
  bla bla bla
\end{abstract}

\section{Introduction}
\label{sec:intro}

Some languages are easier, but how much performance do we lose by
using them?

Purposes: determine which language we can tell our students to
use to implement routing optimization routines. Also determine which
language we can use ourselves.

\subsection{Scope of this study}
We want to compare the reasonably efficient implementation of various
algorithms used in combinatorial optimization. In particular, we look
at five algorithms that are commonly used in vehicle routing
optimization. However these algorithms have features that are also
present in other domains of optimization, such as modifying
subsequences, inserting and removing elements in a sequence, or other
operations on graphs.

Our goal is not to provide the most efficient implementation of each
algorithm for a set of languages, or even to use the most efficient
algorithm to solve the given problems, but rather to use these
algorithms as benchmarks, each capturing certain features currently
encountered in optimization. We put ourselves in the situation of an
average student having to implement these algorithms, and compare the
performance of different programming languages in that context. We
also assess the impact on performance of some implementation
decisions, e.g. using a flat matrix representation.

\subsection{What we measure}
In all proposed benchmarks, we measure the CPU clock time of the
benchmarked algorithm. Every other task, such as reading input data or
constructing an initial solution for local search benchmarks, is not
measured. However if an auxilliary graph needs to be constructed every
time the algorithm is run, then we also measure the construction of
said auxilliary graph. This is the case for benchmarks \emph{maxflow}
and \emph{espprc}.

In general, compilation time is not measured. However, some
implementations do just-in-time (JIT) compilation, for example Julia,
JavaScript, Pypy, Numba and Java. In such cases, this JIT compilation time is
included in the running time of the algorithm. In general we measure
clock time, i.e. the CPU budget used by the algorithm. In the case of
JavaScript we measure wall time as we are not aware of any
way to measure clock time in JavaScript.

In order to represent a tour in heuristics we use variable-size
vectors. Dynamic size containers are necessary in most algorithms for 
vehicle routing, for instance to insert or delete a vertex from a
tour.
% This is the reason why we do
% not provide a C implementation. It is of course possible to
% implement variable-size vectors using fixed-size arrays and
% bookkeeping; however this stands directly against our stated purposes
% and philosophy. Moreover, implementing a bug-free, efficient version
% of variable-size vectors is unlikely to result in something better
% than existing libraries.
Inserting and deleting elements in a vector has linear worst-case time
complexity. However, for heuristics it is still usually a good idea to
use vectors rather than, say, linked lists, since neighborhood
exploration typically requires to evaluate many moves then perform
only one of these moves. The complexity of performing a move is thus
secondary and unlikely to be a performance bottleneck. Additionally,
performing certain moves can actually be more efficient using a
vector than using a linked list, e.g. inverting a subsequence in the
context of 2-opt.

\subsection{Limitations}
Benchmarks are by definition limited in scope; any given benchmark
measurement is only valid within a certain environment. In our case
such environmental factors include the choice of compilers, compiler
versions, operating system and CPU family. Therefore all results are
to be taken as indicators but not absolute truths. The intent of this
study is not to declare a language the winner because it is
consistently faster than others by a few percents, but rather to
assess what kind of performance losses can be expected by, for
example, using Python instead of C++. Another example is to assess what kind of
performance gains can be achieved by using a JIT compiler for Python. 

\section{The benchmarks}
\label{sec:benchmark}
We consider five different benchmark algorithms that perform tasks
commonly encountered in transport optimization. Together they capture
key aspects of computationally intensive tasks in heuristics and
exact methods for transport optimization, although they do not
necessarily use the most efficient algorithm to solve the problem they
are tackling. Additionally, we believe
that they also capture such aspects for other fields of application
of operations research, such as e.g. scheduling. For the sake of
simplicity, we consider the symmetric travelling salesperson problem
(TSP) as a base problem, i.e. input data are in the form of a
symmetric distance matrix while a solution is a permutation. However
some benchmark algorithms solve different problems than the TSP based on these
TSP data, as described below.

All five benchmarking algorithms described below are deterministic.
Additionally, the input provided to them is also deterministic,
i.e. each implementation receives the exact same inputs and performs
the same operations from these inputs. For that purpose, we generate
various instance files, each with a given size $n$, which represents
the number of vertices in the graph, and a number $p$ of
permutations. A permutation is a solution to the TSP.
Vertex coordinates are generated randomly in $[0, 100)$. Let $d_{ij}$
be the Euclidean distance between $i$ and $j$, then the distance
considered is $c_{ij} = \lfloor 100 d_{ij} \rfloor$. Since it is an
integer number, it is sufficient to use integer number representation
to compute the cost of a TSP solution; however certain benchmarks use
floating-point number representation, as explained below.
Each instance
file contains one $n \times n$ distance matrix as well as $p$ randomly
generated permutations, which are used as starting \emph{seeds} for
the algorithms (e.g. as starting solution for 2-opt).

Since the same instance files are given to 
each implementation, all implementations perform the same operations
and return the same result. In order to control result integrity we use a
mechanism similar to \emph{checksum}. The checksum calculation for
applying a certain algorithm using a given permutation as starting seed differs
based on the algorithm and is explained separately for each algorithm
below. The checksum calculation for an instance file is the sum of
checksum values over all permutations in that instance file.

Performances are measured per instance file, i.e. each time reported
is for running an algorithm $p$ times, using one distance matrix with the $p$
different seed permutations.

\subsection{2-opt}
The 2-opt heuristic was first described by~\cite{2-opt}.
One of the most commonly used heuristics in vehicle routing, 2-opt
improves a tour by performing 2-exchanges. A 2-exchange consists in
removing two edges from a tour and reconnecting the tour with two
other edges. It is equivalent to inverting a sub-sequence of the
tour, and can be performed in place using an array or vector solution
representation. If the distances are symmetric, then each move can be
evaluated in constant time. 

The checksum for a given permutation seed is calculated as the number of
improvements found while applying first-improvement 2-opt until no
improving move exists, using this permutation as starting solution.

\subsection{Or-opt}
Or-opt is a heuristic that was first introduced by~\cite{Or-opt}.
It relies on the exploration of a neighborhood that is a subset of all
3-exchanges. A 3-exchange consists in removing 3 edges from a tour and
reconnecting the tour in a different way. Of all these moves, Or-opt
only considers those that shift a sequence of 1, 2 or 3 vertices to a
different position in the tour. Each move can be evaluated in constant time. 

The checksum for a given permutation seed is calculated as the number of
improvements found while applying first-improvement Or-opt until no
improving move exists, using this permutation as starting solution.

\subsection{LNS: Large neighbourhood search}
Large neighborhood search (LNS) was originally introduced in
conjunction with constraint programming to solve the vehicle routing
problem with time windows (VRPTW)~\cite{lns}. Since it has been
a very popular method for tackling a wide variety of vehicle routing
problems, see e.g.~\cite{lnschapter}.

LNS iteratively (i) copies the \emph{incumbent} solution, (ii)
destroys the copy by removing some vertices from it, (iii) repairs the
partial solution by re-inserting previously removed customers in it
and (iv) determines whether this newly produced solution becomes the
new incumbent solution or not. The method relies, among other
things, on randomness. This is an issue since we want each benchmark
to be deterministic. To remedy this issue, we design deterministic
schemes for both destroying and repairing stages. For each starting
seed, we apply a certain number of LNS iterations. At each iteration,
the vertices at even indices in the tour are removed. These removed
vertices are stored in a vector in the order in which they were
removed, i.e. from smallest to largest index. Then the cost of each
possible insertion is calculated and the cheapest insertion is
performed. Because removed vertices are stored in a vector, the
algorithm's behaviour in case of same-cost insertions is
deterministic. 

The checksum for a given permutation seed is calculated as the total
cost of insertions performed while applying 10 iterations of LNS using
that permutation as starting solution.

\subsection{Dynamic programming for column generation}
\label{sec:espprc}
Many state-of-the-art exact methods for vehicle routing problems rely
on column generation (see e.g.~\cite{Baldacci:2012survey}). In such
methods, a significant amount of CPU effort is spent solving the
\emph{pricing subproblem}, which differs depending on the problem at
hand but is nonetheless often an elementary shortest path problem with
resource constraints (ESPPRC). While it can be solved heuristically,
in order to establish optimality it eventually needs to be solved
exactly as well. For this benchmark we implement a dynamic
programming algorithm similar to the one by~\cite{feillet:2004exact}
to solve the ESPPRC, albeit without time windows. One difficulty in
this problem stems from the existence of negative-cost cycles in the
graph. 

In order to simulate an environment similar to the one encountered in
column generation for VRPs, we derive an ESPPRC instance from each TSP
solution by using an auxilliary distance matrix $c'$ from the
distance matrix $c$, as follows:
\begin{enumerate}
\item For each vertex $j$, let $\pi_j$ be the cost of arc $(i, j)$,
  where $i$ is the predecessor of $j$ in the TSP solution.
\item For each arc $(i, j)$, set $c'_{ij}$ to $c_{ij} - d_j$.
\end{enumerate}
Additionally, we generate resource consumption as follows: we consider
6 resources numbered from 0 to 5, and any vertex $i$ consumes resource
$r$ iff the $r^{th}$ bit of $i$'s binary representation is 1
(considering that the first bit is bit 0). That is, in C syntax, iff
\verb|i & (1 << r) > 0|. Capacity is set to 1 for each resource.
The algorithm then computes the elementary shortest path with resource
constraints on $c'$ from 0 to 0 (1 to 1 in Julia).

For this benchmark, floating-point numbers are used to represent cost,
even though the values are integer. This is to emulate the usual
setting when solving such problems.

Each label needs (i) a reference to its predecessor, so that
paths can be reconstructed and (ii) a collection of references to
its successors, in order to recursively propagate dominance and later
ignore labels that are successors of an already proved-to-be-dominated
label. Rust does not allow that, at least not in the intuitive way
where a label is encapsulated in a \verb|struct|, including references
to predecessor and successors. This is because Rust is focused on
security, while this type of construct can lead to unsafe operations if
not used properly. This means that this benchmark cannot be
implemented with Rust, even though it is consistent and reliable with all
other implementations. In order to remediate this issue, we also
implement a second benchmark to implement the same algorithm, although
in a less intuitive way: all labels are stored in a collection, and
the indices in this collection are used in other collections to store all
predecessors and successors. This second implementation represents a
different benchmark, which we call \emph{espprc-index}, whereas the
base benchmark is called \emph{espprc}. The version with indices is more
tedious to implement, but it may also be more efficient, so we also
implement it in other languages.

The checksum for a given permutation seed is the cost of the shortest
elementary path obtained when using that permutation seed to generate the
ESPPRC instance, truncated to its integer part.

\subsection{Maximum flow problem}
Another popular framework for the exact solution of routing
problems is \emph{branch and cut}. Branch and cut is typically applied
to solve mixed-integer linear programs; it is akin to branch and bound
but only considers a subset of all constraints explicitly. Other
constraints are generated dynamically when they are found to be
violated while exploring the search tree. These constraints are said
to be \emph{separated}. One early success story of branch and cut is
actually in application to the symmetric TSP~\citep{padberg:532}, by
separating subtour elimination constraints. The separation procedure
involves finding a minimum capacity cut, which itself involves solving
a maximum flow problem. For this reason the maximum flow problem is
important in vehicle routing, and we dedicate one benchmark to solving
it. The \emph{maxflow} benchmark implements the Edmonds-Karp
algorithm~\cite{edmonds-karp}, which is a specific version of the
Ford-Fulkerson~\cite{ford-fulkerson} augmenting path method. The
specificity resides in how the augmenting paths are generated.

We derive a capacity graph $C$ from the distance matrix $c$ and a
given TSP solution as follows: 
\begin{enumerate}
\item For each vertex $j$, let $t_j$ be the cost of arc $(i, j)$,
  where $i$ is the predecessor of $j$ in the TSP solution.
\item For each arc $(i, j)$, set $C_{ij}$ to $c_{ij}/1000$ if $c_{ij}
  > t_j$, otherwise set it to 0.
\end{enumerate}

For any starting seed, the capacity graph is generated then the
Edmonds-Karp algorithm is applied with 0 as \emph{source} and every other
node as \emph{sink}. The checksum for that permutation seed is the sum
of all maximum flow values thus obtained, truncated to its integer part.

The reason why capacity values are distance values divided by 1000 is
that otherwise, checksum values can exceed the largest possible
integer representation with 32 bits, which can be a problem with some
languages. This means that floating-point numbers are used, which is
representative of the use case of branch and cut for vehicle routing problems.

\section{Languages and implementations considered}
We consider six different programming languages: C++, Python, Java,
Julia, Rust and Javascript. Any rule of selection
is arbitrary in nature; however we indicate below, for each of these
languages, our perceived advantages of these languages, which justify
their selection. It is possible to add other languages to this list in
the future, and keep the benchmark collection alive in an online form.
For a given language several implementations are possible, e.g. one
with nested distance matrices and one with flat distance matrices. As
a convention, implementations are named in lower-case letters, for
example \emph{python-flat-matrix}. We also use the simplest names for
the best implementations, i.e. \emph{python} uses a nested distance
matrix because it performs better than \emph{python-flat-matrix}, but
\emph{pypy} uses a flat matrix because it performs better than
\emph{pypy-nested-matrix}. All implementations are summarised in
Appendix~\ref{app:impl}.

For each language, we may consider multiple \emph{implementations},
which differ by which mechanisms or library they use. For
instance with Python we can compare an implementation using native
lists against an implementation using Numpy arrays. Implementations are
detailed individually below for each language. The code for all
implementations can be found at
\url{https://github.com/fa-bien/routing-benchmark}.

It is difficult to properly measure the popularity of a programming
language. There exist rankings, for example based on online
searches~\cite{TIOBE} or on GitHub contributions~\cite{octoverse}. All
these approaches have flaws. Basing an index on online searches is
subject to all the biases and exploits that target online
search. Measuring GitHub contributions is not clear, as
there is no trivial way to decide what should count among
repositories, number of lines written, number of commits,
etc. Ultimately, language popularity for its own sake does not need to
be a crucial factor in the choice of a programming language, especially for
academic research. These aspects seem more important to us:
\begin{itemize}
\item The programs written in a given programming language should be
  easily run on any computer. For that reason, we only look at
  languages that are free and work under multiple operating
  systems. The ability to run under Linux is especially important,
  since it is typically the operating system of high-performance
  computing (HPC) clusters~\cite{top500}.
\item The language should be well documented, but also have online
  resources for support. This in fact also depends on popularity.
\end{itemize}

\subsection{C++}
C++ is one of the most popular programming languages in the World, and
has been so for decades. Among its many features there is good
runtime performance. In fact the performance is good enough that whole
operating systems can be written in C++, for example Microsoft
Windows. In fact C++ has so many features that it is often possible to
implement the same algorithm using different subsets of the language,
and a frequent criticism is that people and teams evolve into using
only their own subset of the language.

We develop two C++ implementations of every benchmark, using the C++98
and C++14 standards, which we call \emph{c++98} and \emph{c++14}. they
both use a flat distance matrix. With C++98 we use references to
objects, while with C++14 we use smart pointers such as
\verb|shared_ptr|. Comparing these two implementations will allow us
to evaluate the cost of smart pointers.

For each benchmark the reference CPU time is provided by the c++98
implementation. We also develop a variant of c++14 using a nested
distance matrix, and another using static arrays.

\subsection{Python}
Python is another of the most popular programming languages in the
World, especially for tasks of machine learning in recent years. The
base Python implementation is called \emph{python} and uses a nested
distance matrix. There is also an implementation using a flat distance
matrix, called \emph{python-flat-matrix}, and one using a nested
distance matrix wrapped in function calls, called
\emph{python-nested-matrix-function}.

One issue with Python is that much of the code is not compiled, rather
directly interpreted, which can lead to poor performance. One way to
address that issue is to rely on libraries that are implemented in C,
however that is not always possible. We consider alternative
implementations in Python, all bringing different solutions to this
issue.

The simplest way to speed up the run time of a Python program is
to run this program with Pypy~\cite{pypy} instead of the
standard CPython interpreter. Pypy uses JIT compilation to speed
things up. There is no need to modify the Python program, which is a
strong advantage compared to library-based solutions or
annotation-based solutions. Pypy's website claims that it is on
average 4.2 times faster than CPython~\cite{pypy}, however this is
just an average and actual numbers may vary a lot depending on the
benchmark. We use both the python and python-flat-matrix
implementations with Pypy, resulting in \emph{pypy-nested-matrix} and
\emph{pypy} implementations, respectively.

We also implement the benchmarks using Numpy arrays instead
of standard Python lists. Numpy is one of the most popular external
libraries for Python; it is coded in C for runtime
performance~\cite{numpy}. Numpy performs especially well when using
\emph{array operations}, for instance matrix multiplication. However
the benchmarks that we consider do not take advantage of Numpy
functions, only of its array data structure. Since Numpy provides
multi-dimensional arrays, we use a two-dimensional array to represent
the distancematrix. Additionally, we consider a flat matrix in
implementation \emph{numpy-flat-matrix}.

Numpy itself does not provide any kind of JIT compilation, but another
Python package, Numba, performs JIT compilation when using Numpy
arrays. Numba requires code annotation, therefore we annotate the
Numpy implementations to take advantage of them with Numba.

\subsection{Java}
Java is a very popular programming language of the past two
decades. Java code is compiled to \emph{bytecode}, which is then run
by a virtual machine (VM). The bytecode compiled on one computer can
be run by a VM on another computer. Nonetheless, JIT compilation to
native code happens dynamically at run time. The syntax is based on
C/C++ syntax. The base Java implementation, \emph{java}, uses a flat
distance matrix and a \verb|ArrayList<Integer>| object to represent a
tour. We also implement a version with nested distance matrix,
\emph{java-nested-matrix}, as well as a version with static arrays to
represent a tour, \emph{java-static-arrays}.

\subsection{Julia}
Julia is a relatively recent programming language. It is a
general-purpose language but it is also particularly aimed at
computationally intensive tasks such as numerical
analysis~\cite{Julia-2017}. Version 1.0 was 
released in 2018 and the language is well-documented and has a strong
community.

Compilation happens in a JIT fashion. Like Numpy but
unlike all other programming languages considered, Julia supports
multi-dimensional arrays, therefore a native two-dimensional array can be
used to represent a distance matrix. We also develop an
implementation using flat matrix representation in Julia, called
\emph{julia-flat-matrix}; we will determine which version is better
through experiment.

One valuable feature of Julia is the fact that projects have an
environment, including package dependencies with specific
versions. The language makes it easy to \emph{instantiate} the
project environment, i.e. recreate the same environment in which the
project was developed, with the same packages in the same
versions. This provides safety from new library versions which break
backward compatibility.

\subsection{Rust}
Rust is also relatively recent, with version 1.0 released in 2015. It
is a general-purpose language with an emphasis on memory safety and
on performance. Notable software using Rust include Firefox, Dropbox
and Cloudflare~\cite{rust-lang}.

Programs in Rust are pre-compiled, like in C++ for
instance. Like Julia, the language is well documented and has a strong
community. Also like Julia, projects have an environment, and it is
easy to recreate the environment for a given project, including
dependencies and their specific version, which again provides safety
from backward-compatibility-breaking library updates. The \emph{rust}
implementation uses a flat matrix representation.

\subsection{JavaScript}
JavaScript is likely one of the most popular programming languages in
the World, as it is used on countless websites as well as mobile
applications based on web technology. JavaScript is not expected to
perform as well as, say, C++. However there are several reasons why it
can be appealing to implement routing algorithms in JavaScript:
\begin{itemize}
\item Any program written in JavaScript can run on virtually any
  computer with a modern web browser, including smartphones.
\item Integrating JavaScript with a web interface is especially easy.
\item JavaScript engines have received considerable attention from
  major companies and been the subject of fierce competition in
  relation with web browsers engines. As a result they have seen vast
  performance improvements over the years and the trend is likely to continue.
\end{itemize}

We use Node.js, which allows to run JavaScript programs from the
command line. This means that there is in fact not even the need for a
web browser. Node.js currently uses Google Chrome v8's JavaScript
engine~\cite{nodejs}.

The \emph{javascript} implementation uses a flat matrix representation
but there is also an implementation using nested matrix representation,
called \emph{javascript-flat-matrix}.

\subsection{Considerations on how to implement a distance matrix}
Perhaps the most straightforward way to implement a distance matrix in
a number of languages is to use nested arrays, i.e. each element of the
main array is an array representing a row of the distance matrix. This
is typically done in C++, Java, Python (although the structure is
officially called a \emph{list} and not an array), Javascript. C and
C++ use pointers to achieve that effect. All these languages use the
same C syntax for looking up values in the distance matrix: the
distance between vertices $i$ and $j$ using distance matrix $d$ is
written \lstinline{d[i][j]}.

Another easy way to implement a distance matrix is to use what we
call from here on a \emph{flat} distance matrix representation, which
is a single-dimensional array containing all distance values. Assuming
indices start at 0, the distance from $i$ to $j$ in array $d$ can be
coded as \lstinline{d[i*n+j]}, where $n$ is the total number of
vertices. This representation guarantees that the whole distance
matrix is stored in contiguous memory. Additionally, looking up in
arrays also takes time, so one lookup is better than two. The drawback
is that we have to pay a multiplication and an addition for each
lookup, and that lookups have to be wrapped in a function call for the
sake of readability. Such function calls can usually be inlined,
i.e. the function content is substituted to the function call, so
there should be no runtime penalty from using a function call. In
general we expect a speedup from using a flat representation. We
implemented some of the benchmarks with both flat and nested matrix
representation in order to determine if there are significant
performance differences.

There are a few cases where the above considerations on using a flat
representation do not apply: Python does not allow inlining, while
Numpy and Julia provide multi-dimensional arrays. Nonetheless we also
wrote a flat matrix version of these implementations, in order
to elicit any difference in performance. 

\subsection{Other general cross-language considerations}
Here we discuss considerations that are subjective in nature, as they
relate to individual experience implementing all benchmarks in all
languages. Experience using various languages prior to the beginning
of this work needs to be taken into account:
\begin{itemize}
\item C++: numerous projects over 20 years.
\item Python: numerous projects over 15 years; no experience of Numpy
  or Numba.
\item Java: PhD work 15 years ago, one minor project 5 years ago.
\item Julia: one ongoing research project started 3 years ago.
\item Javascript: a few hobby projects 5-7 years ago.
\item Rust: no experience at all.
\end{itemize}
To add to the subjectivity, each benchmark is first implemented in
Python then in other languages, with Rust typically being the last one
to be implemented. Therefore the languages implemented last represent
supposedly easier tasks, due to increased familiarity with the
algorithm. For all reasons mentioned above, measuring the
time spent implementing each benchmark in each language would be
meaningless, therefore we limit ourselves to subjective opinion here.
Keeping this in mind, we see a subset of perceived
\emph{easy} languages, as in ``easy to program in and get things running
with the expected outcome'', which consists of Python, Julia and
JavaScript. These 
easy languages typically allow to program the same algorithms faster
than the other ones with the same results, performance considerations
aside.
On the other hand, we found the process of getting benchmarks to work
with Rust to be much more tedious than with any other language,
although this is certainly due to lack of experience. Nonetheless, the
JavaScript implementation was typically easier than the C++ or Java ones
despite having more experience with C++ and Java, so
experience does not explain everything.
Ultimately, there is little doubt that any experienced programmer in
any of these languages would have no trouble implementing any of these
benchmarks. Nonetheless, this programmer with experience in mostly C++
and Python implemented every benchmark faster in Python, JavaScript or Julia
than in C++.

We also want to underline that no matter the language, it is possible
to write both efficient and inefficient code. Familiarity and
experience are beneficial; beginners in a language are more likely to
write inefficient code. This being said, not every language is equal
when it comes to documenting how to write efficient code. We note that
the official Julia documentation has a whole section dedicated to that
topic~\cite{julia-performance}. There also exists an automated tool to
detect inefficient code patterns, based on the official
documentation~\cite{julia-traceur}.

\section{Implementation-specific notes}
\subsection{Python}
Python does not allow to inline functions. Since using a flat matrix
requires to wrap matrix value lookup in a function, using a flat distance
matrix in Python is not beneficial. It would of course be possible to
inline by hand, i.e. formulate the correct 1-dimensional index in the
matrix for every lookup, but that would be impractical and defeat the
purposes and philosophy stated in Section~\ref{sec:intro}.
Running Python code that uses a flat representation with a JIT compiler
(e.g. Pypy, Numba) introduces automatic inlining and is likely to
remedy this.

On the ESPPRC benchmark, a label needs to store information about
which customers have already been visited in the partial path it
represents. In Python, preliminary testing reveals that using a set of
integers for that purpose is twice as fast as using a vector of
booleans. Similar preliminary testing in C++ reveals the opposite:
using an array of booleans is faster. This is the behaviour we would
expect in general, the observation on Python are the surprising ones
here. There are two reasons for this expectation: (i) copying an array,
represented in contiguous memory, can in general be performed faster
than copying a set structure and (ii) lookup in an array is performed
in $\mathcal{O}(1)$ while lookup in a set is performed in
$\mathcal{O}(log(n))$.

\subsection{Julia}
Julia has native multi-dimensional arrays, which may be more efficient
than a flat representation. We will determine which version to use
based on experiment. Since Julia's arrays start with index 1, the code
for flat representation lookup of the distance between vertices $i$
and $j$ in matrix $d$ is \lstinline{d[i*n-n+j]}. It is worth noting
that it involves one subtraction on top of the addition and
multiplication used for 0-indexed languages.


% Using two threads instead of 1 improves performance by about 10\%,
% although it is unclear what the second thread is used for.


\subsection{JavaScript}
All benchmarks are repetitive in nature, as they successively perform similar
operations with different input data. In JavaScript, when our
implementation loads 40 instances and performs the same
benchmarking operations on all 40 them, we observe a significant performance
hit after a few instances (2-5 times slower). In order to remedy this,
we re-start the program individually for each instance. Advanced
knowledge of this specific JavaScript engine might allow to remove that
performance hit, but this is clearly outside the scope of this
study. However it is worth noting that this can be a concern in general.

\subsection{Numba}
Numba is not yet feature-complete for Numpy. A direct consequence is that some
benchmarks cannot be implemented for Numba. For example
\verb|numpy.insert()| is not implemented, but this function is
needed for the $LNS$ benchmark, therefore the $LNS$ benchmark cannot be
implemented with Numba. Additionally, we were not able to implement
the ESPPRC benchmark in a satisfying manner. In our experience, Numba
error messages are sometimes unrelated to the issue causing them, or
too obscure to make sense, keeping in mind the premises of this
paper. It would certainly be possible to implement a dynamic
programming algorithm for the ESPPRC benchmark using Numba's JIT
compilation features, however our conclusion is that it would not be
possible to do so without advanced knowledge of Numba.

Still, Numba is significant enough to be
benchmarked and its current performances are an indication of what to
expect in years to come, as the project matures.

\subsection{Java}
The ecosystem of Java implementations and versions can be hard to
navigate. There exist multiple compilers, multiple virtual
machines, and there is a new Java version every 6 months, sometimes
breaking backwards compatibility. The official reference
implementation is OpenJDK since version 7~\cite{openjdk-reference}.
We use OpenJDK version 11, which is the current long-term support version as
of writing this article.

\subsection{Rust}
As mentioned in Section~\ref{sec:espprc}, Rust does not allow a given
\verb|struct| to have multiple references to the same \verb|struct|
type. For this reason, we needed to develop a different implementation
of the espprc benchmark. In general Rust puts restrictions on what
constitutes valid code, for the sake of safety. This is a valuable
feature in systems programming, which is the main application that Rust was
designed for. This also means that writing code that compiles and runs
is actually a more tedious task, but that once the code compiles,
there is also a higher chance that it will run according to
expectations, like with the Ada programming language for instance.

In the context of this study, these benefits did not appear relevant
however: each benchmark was programmed in all other languages as well,
providing the same results as with the Rust implementation, while no
evidence of memory leak was observed. As mentioned above, The Rust
implementation was typically way more time-consuming than other
implementations, although one reason for that is lack of
experience. Nonetheless, in the context of this study, the generally
assumed benefits of using Rust are not apparent, with the notable
exception of good runtime performance.

\section{Experiments}
All programs are run concurrently on an desktop computer with an AMD
Ryzen 5 3600 6-Core Processor at 3.6 GHz with 16 GB RAM, running
Linux. Twelve threads can
run in parallel, however in our setting at most 5 threads are used at
any given time. For every benchmark and every instance, CPU times are
reported as a ratio of a \emph{reference time} for that benchmark and
instance. In practice, for each benchmark and instance, the reference
time is the time required by the c++98 implementation for that
benchmark and that instance. Therefore the c++98 implementation always
has a CPU ratio of 1.

A summary of all versions used for compilers and interpreters is
provided in Appendix~\ref{app:lang}.

\subsection{Benchmark data}
Each instance file contains one $n \times n$ distance matrix as well
as $p$ randomly generated permutations, which are used as starting
\emph{seeds} for the algorithms (e.g. as starting solution for 2-opt).
For $n$ taking each value of $\{20, 40, 60\}$ and fixing $p = 1000$ we
generate 40 instance files, for a total of 120 instance files. Each
instance file contains 1000 permutations, therefore is run 1000
different times for each benchmark. As mentioned in
Section~\label{sec:benchmark} distances are Euclidean distances multiplied
by 100 and truncated to their integer part. In the following each data
point represents the total CPU time for running one implementation
for each of the 1000 permutations of one given instance file.

\subsection{A comparison of Python implementations}

\subsection{Impact of using a flat matrix}

\subsection{Impact of using static arrays}

\subsection{A comparison of C++ implementations}
Also add a clang++ vs g++ comparison?

\subsection{General cross-language comparison}

\subsection{Cross-language comparison: fast languages}

\subsection{Cross-platform comparison}
We now look at the same benchmarks, using the same languages, but running
in a different environment. We look at runs on an laptop Intel CPU
(Intel i5-8265U, 4 cores, 1.6 GHz), on a Raspberry Pi 4 (ARMv7,
4 cores, 1.5 GHz), on a shared cluster (Intel Xeon E5-2650 v3, 20
cores, 3 GHz) and on the previously mentioned AMD CPU. The
goal is not to observe which CPU provides the best performance, rather
to determine whether the experimental observations made with one CPU
are valid with the other. For this reason, we only report results that
are different from the ones observed with the previous CPU.
We note here that despite our best effort, language and compiler
versions might vary between platforms, due to the diversity in
package version availability on different systems. Ultimately, the
goal of this analysis is not to obtain a very precise assessment of
relative performance depending on compiler version and architecture, as it
is of limited use to anyone using a slightly different architecture or
compiler version; rather, we want to assess the reliability of
previously observed results.

\section{Discussion}
The question of what programming language to use when implementing
optimization algorithms is one that, by essence, does not have a final
answer. This being said, there is value in knowing how much
performance is gained or lost when using a certain
language on a certain architecture, or how much can be
gained by using different data structures. The benchmark suite
introduced in this paper is free software and can be used for that
purpose. We encourage students and researchers in
transport optimization to use it to support their implementation
decisions. Further experimental analyses similar to the ones conducted
here can also be good practice, for instance simply comparing the
runtime performance of the same C++ code when compiled with different
compilers, which is easier than it ever was.

Additionally, some insight can already be taken straight out of the
experiments conducted above. Using languages like Julia or Rust
does not incur a significant runtime penalty when compared to more
established languages like C++, and can actually be faster
sometimes. Interpreted languages like Python or JavaScript nowadays
have reasonably good runtime performance, under five times slower than
pre-compiled languages, under the right conditions (using a JIT
compiler). This means that the flexibility and ease of use of these
languages can be worth the performance hit, depending on the
application. Additionally, using JavaScript opens perspectives, as the
integration of optimization algorithms in web applications is easier
than it ever was, and can now be run client-side.

\section*{Acknowledgements}
The author is grateful to Sebastian Leitner for his valuable support
in relation with the Rust implementation.

\bibliographystyle{plain}
\bibliography{benchmark}

\appendix

\section{Summary of implementations}
\label{app:impl}
\begin{table}
  \begin{tabular}{l|llll}
    \toprule
    Implementation & Language & Extra & Distance matrix representation
    & Note \\
    \midrule
    \bottomrule   
  \end{tabular}
  \caption{Overview of all implementations}
\end{table}

\section{Summary of language and compiler versions}
\label{app:lang}
\begin{table}
  \begin{tabular}{l|l}
    \toprule
    Language/tool & language or compiler version & options \\
    \midrule
    C++14 & g++ 10.2.1 & -Wall -ansi -pedantic -O3 -std=c++14 \\
    C++98 & g++ 10.2.1 & -Wall -ansi -pedantic -O3 -std=c++98 \\
    CPython & 3.9.2 & \\
    Pypy & 7.3.3 & \\
    % Numpy & & \\
    % Numba & 0.53.1 & \\
    Java & OpenJDK version 11 & \\
    Julia & 1.6.0 & --check-bounds=no --inline=yes -O3 -t 1 \\
    Rust & 1.51 & cargo build --release \\
    JavaScript & Node & \\
    \bottomrule   
  \end{tabular}
  \caption{Overview of all implementations}
\end{table}

There are small variations in versions used on different
platforms. Most notably, the Julia version used on the ARMv7 CPU is
1.3, as it is the latest version available for that architecture.

\end{document}